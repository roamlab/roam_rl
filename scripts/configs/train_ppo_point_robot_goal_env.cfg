[experiment]
experiment_no = 0
robot_name = point_robot_env
logging = logging
ppo = my_ppo

[my_ppo]
env_maker = my_env_maker
vec_env_maker = my_vec_env_maker
network = mlp
total_timesteps = 1e6
lr = lambda f: 3.0e-4 *f
nsteps = 1024
log_interval = 1
value_network = copy
nminibatches = 32
noptepochs = 10
num_hidden = 64
num_layers = 2
seed = 0

[my_vec_env_maker]
type = shmem
nenvs = 1
normalize = False

[my_env_maker]
entrypoint = roam_rl.env.utils.env_maker:WrapObsEnvMaker
observation_wrapper = FlattenDictWrapper
observation_keys = ['achieved_goal', 'desired_goal']
env = my_robot_env

[my_robot_env]
entrypoint = roam_rl.robot_env:RobotGoalEnv
robot_world = my_robot_world
state_sampler = my_robot_env_state_sampler
observation_func = my_robot_env_observation_func
reward_func = my_robot_env_reward_func
goal_sampler = my_robot_env_goal_sampler
max_episode_steps = 100

[my_robot_world]
entrypoint = roam_robot_worlds.robot_world:SimulatedRobotWorld
dynamics = my_robot_world_dynamics
sensor_model = my_robot_world_sensor_model
steps_per_action = 100

[my_robot_env_state_sampler]
entrypoint = roam_rl.robot_env.components:UniformSampler
max = [1.0, 1.0]
min = [-1.0, -1.0]

[my_robot_env_goal_sampler]
entrypoint = roam_rl.robot_env.components:UniformSampler
max = [2.0]
min = [-2.0]

[my_robot_env_observation_func]
entrypoint = roam_rl.simple_robot_envs.point_robot_goal_env:ObservationFunc

[my_robot_env_reward_func]
entrypoint = roam_rl.robot_env.components.goal_reward_func:DenseGoalRewardFunc
achieved_goal_reward = linear
action_reward = none
alpha = 1.0

[my_robot_world_dynamics]
entrypoint = roam_robot_worlds.simple_robots.point_numpy_robot:PointNumpyDynamics
mass = 1.0
delta_t = 0.001

[my_robot_world_sensor_model]
entrypoint = roam_robot_worlds.robot_world.sensor_model:StateSensorModel

[logging]
enable_detail_logging = False
env_base_log_level = 12
env_detail_log_level = 11
script_base_level = 12
script_detail_level = 11
script_debug_level = 10
script_console_level = 12