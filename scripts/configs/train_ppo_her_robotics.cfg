[experiment]
experiment_no = 0
robot_name = FetchReach-v1
logging = logging
ppo = my_ppo

[my_ppo]
env_maker = my_env_maker
vec_env_maker = my_vec_env_maker
network = mlp
total_timesteps = 2.0e5
lr = lambda f: 3.0e-4 *f
nsteps = 1024
log_interval = 1
value_network = copy
nminibatches = 32
noptepochs = 10
num_hidden = 64
num_layers = 2
hs_strategy = split10

[my_vec_env_maker]
type = subproc
nenvs = 2
normalize = False

[my_env_maker]
type = wrap_obs_env_maker
robot_env = my_robot_env
observation_wrapper = DictInputWrapper
observation_keys = ['observation', 'achieved_goal', 'desired_goal']

[my_robot_env]
type =  openai_env
id = FetchReach-v1
reward_type = dense

[logging]
enable_detail_logging = False
env_base_log_level = 12
env_detail_log_level = 11
script_base_level = 12
script_detail_level = 11
script_debug_level = 10
script_console_level = 12
