[experiment]
experiment_no = 0
robot_name = point_robot_env
logging = logging
ppo = my_ppo

[my_ppo]
env_maker = my_env_maker
vec_env_maker = my_vec_env_maker
network = my_network
total_timesteps = 1e6
lr = lambda f: 3.0e-4 *f
nsteps = 1024
log_interval = 1
value_network = copy
nminibatches = 32
noptepochs = 10
seed = 0

[my_network]
type = mlp
num_hidden = 64
num_layers = 2

[my_vec_env_maker]
type = shmem
nenvs = 1
normalize = False

[my_env_maker]
entrypoint = roam_rl.env.utils.env_maker:WrappedEnvMaker
wrappers = ['my_wrapper_1', 'my_wrapper_2']
env = my_robot_env

[my_wrapper_1]
entrypoint = roam_rl.env.utils.wrappers:FilterObservation
filter_keys = ['state']

[my_wrapper_2]
entrypoint = roam_rl.env.utils.wrappers:FlattenObservation

[my_robot_env]
entrypoint = roam_rl.robot_env:RobotEnv
robot_world = my_robot_world
state_sampler = my_robot_env_state_sampler
observation_func = my_robot_env_observation_func
reward_func = my_robot_env_reward_func
max_episode_steps = 100

[my_robot_world]
entrypoint = roam_robot_worlds.robot_world:SimulatedRobotWorld
dynamics = my_robot_world_dynamics
sensor_model = my_robot_world_sensor_model
steps_per_action = 100

[my_robot_env_state_sampler]
entrypoint = roam_rl.robot_env.components:UniformSampler
max = [1.0, 1.0]
min = [-1.0, -1.0]

[my_robot_env_observation_func]
entrypoint = roam_rl.simple_robot_envs.point_robot_env:ObservationFunc

[my_robot_env_reward_func]
entrypoint = roam_rl.simple_robot_envs.point_robot_env:RewardFunc

[my_robot_world_dynamics]
entrypoint = roam_robot_worlds.simple_robots.point_numpy_robot:PointNumpyDynamics
mass = 1.0
delta_t = 0.001

[my_robot_world_sensor_model]
entrypoint = roam_robot_worlds.robot_world.sensor_model:StateSensorModel

[logging]
enable_detail_logging = False
env_base_log_level = 12
env_detail_log_level = 11
script_base_level = 12
script_detail_level = 11
script_debug_level = 10
script_console_level = 12